import joblib
import os
import sys
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from py.FILE_HANDLERS.OTHER.date_folders import TIME_FOLDERS
from tqdm import tqdm
from time import sleep

# –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–ø–∫—É py –≤ sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), 'py'))

def LOG_REG_LEARNING(documents, y):
    """
    –ü–æ–¥–±–∏—Ä–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ TF-IDF-–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞, —Ç–∞–∫ –∏ LogisticRegression
    —Å –ø–æ–º–æ—â—å—é Pipeline + GridSearchCV. –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —á–µ—Ä–µ–∑ tqdm.
    """

    pipeline = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('clf', LogisticRegression())
    ])

    param_grid = [

        # –ë–ª–æ–∫ 1: L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, —Ç–æ–ª—å–∫–æ solver = liblinear
        {
            'tfidf__min_df': [2, 3],
            'tfidf__max_df': [0.8, 1.0],
            'tfidf__max_features': [5000, 10000],
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'clf__penalty': ['l1'],
            'clf__C': [0.1, 1, 10],
            'clf__solver': ['liblinear'],
            'clf__max_iter': [300],
            'clf__class_weight': ['balanced', None],
        },

        # –ë–ª–æ–∫ 2: L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, solver = liblinear
        {
            'tfidf__min_df': [2, 3],
            'tfidf__max_df': [0.8, 1.0],
            'tfidf__max_features': [5000, 10000],
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'clf__penalty': ['l2'],
            'clf__C': [0.1, 1, 10],
            'clf__solver': ['liblinear'],
            'clf__dual': [False],  # –≤–∞–∂–Ω–æ!
            'clf__max_iter': [300],
            'clf__class_weight': ['balanced', None],
        },

        # –ë–ª–æ–∫ 3: L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, solver = lbfgs (–±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω—ã–π)
        {
            'tfidf__min_df': [2],
            'tfidf__max_df': [0.8],
            'tfidf__max_features': [8000, 10000],
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'clf__penalty': ['l2'],
            'clf__C': [1, 10, 100],
            'clf__solver': ['lbfgs'],
            'clf__max_iter': [300],
            'clf__class_weight': ['balanced', None],
        }

    ]

    search = GridSearchCV(
        pipeline,
        param_grid=param_grid,
        scoring='accuracy',
        cv=5,
        n_jobs=-1,
        verbose=10  # <-- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ fold'–∞–º
    )

    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–±–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")

    # –ü–æ–∫–∞–∂–µ–º —Ñ–µ–π–∫–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä, –ø–æ–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç .fit()
    # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ —á–∏—Å–ª–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: 108 (—Å–º–æ—Ç—Ä–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–∞—Å—á—ë—Ç)
    with tqdm(total=1, desc="GridSearchCV –æ–±—É—á–µ–Ω–∏–µ", bar_format="{l_bar}{bar}| {elapsed}") as pbar:
        search.fit(documents, y)
        pbar.update(1)

    best_pipeline = search.best_estimator_
    print("‚úÖ –õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:", search.best_params_)
    print("üìà –õ—É—á—à–∏–π score (accuracy):", search.best_score_)

    path_for_mv = r"C:\Users\kovalchuk\PycharmProjects\DOCS_ANALYZE\py\CORE_VECTOR"
    joblib.dump(best_pipeline, filename=os.path.join(TIME_FOLDERS(path_for_mv), "core_pipeline.joblib"))

    train_preds = best_pipeline.predict(documents)
    print("üì¶ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ:", train_preds)
    print("üßæ –ö–ª–∞—Å—Å—ã –º–æ–¥–µ–ª–∏:", best_pipeline.named_steps['clf'].classes_)
